%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt, a4paper]{article}
\usepackage[top=3cm, right=2cm, bottom=2.5cm, left=2cm]{geometry}
\setlength{\parindent}{0pt}

\usepackage{float}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[utf8]{inputenc} 
% \usepackage{fontspec}
\usepackage{textcomp}
\reversemarginpar
\addtolength\marginparwidth{20pt}

\usepackage{ifpdf} % live url's if pdf.
\ifpdf
  \usepackage[breaklinks,colorlinks,linkcolor=black,citecolor=black,
              urlcolor=black]{hyperref}
\else
  \usepackage{url}
\fi


\usepackage{titling} \setlength{\droptitle}{-110pt}
\pagestyle{headings}


\usepackage{graphicx, amsmath,  amssymb, amstext, amsfonts, color,
enumerate, subfigure, listings, longtable, lastpage,
fancyhdr, fancybox, hyperref, algorithmic, mathrsfs,mathabx} %math

\usepackage[parfill]{parskip}


\fancyhf{}
\usepackage{array,ragged2e}
\usepackage{ae}
\usepackage[amsmath,thmmarks]{ntheorem}
\usepackage{dsfont}
\frenchspacing

\newcommand{\authors}{Philipp Müller} % Name
\newcommand{\tutor}{} % Name Deines Tutors
\newcommand{\expage}{} % Die Nummer des Übungsblatts
\newcommand{\expageuntil}{} % Das Abgabedatum
\newcommand{\doctitle}{\textbf{Multivariate Statistik}}
\newcommand{\fach}{}
\newcommand{\E}{\mbox{I\negthinspace E}} 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\doctitle}
\author{\authors}
\date{\today}
\pagenumbering{arabic}
% Kopfzeile
\lhead{\doctitle}
\rhead{\date}

% Fußzeile
%\rfoot{\textbf{Tutor:} \tutor}
%\lfoot{\fach}
%\cfoot{\textbf{Abgabe:} \expageuntil}
\rfoot{Seite \thepage\ von \pageref{LastPage}}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}


\theoremstyle{definition}
\newtheorem{satz}[thm]{Satz}
\newtheorem{theorem}[thm]{Theorem}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{proposition}[thm]{Proposition}
\newtheorem{korollar}[thm]{Korollar}
\newtheorem{beispiel}[thm]{Beispiel}
\newtheorem*{beweis}{Beweis}
\newtheorem{aufgabe}{Aufgabe}

% Now the optional argument takes over

\newtheorem{namedtheorem}{}
% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{lemma}[section]{Lemma}
% \newtheorem{proposition}[section]{Proposition}
% \newtheorem{korollar}[theorem]{Korollar}


% \newenvironment{beweis}[1][Beweis]{\begin{trivlist}
% \item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
% \newenvironment{definition}[1][Definition]{\begin{trivlist}
% \item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
% \newenvironment{beispiel}[1][Beispiel]{\begin{trivlist}
% \item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
% \newenvironment{bemerkung}[1][Bemerkung]{\begin{trivlist}
% \item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \renewcommand{\baselinestretch}{1.2}

\selectlanguage{german}


% some math stuff
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\1}{\mathds{1}}
% \newcommand{\N}{\mathbb{N}}
\renewcommand{\E}{\mathrm{E}}


\newcommand{\Var}{\mathrm{Var}}
\newcommand{\BIN}{\mathrm{BIN}}

\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\diag}{\mathrm{diag}}
% \renewcommand{\C}{\mathbb{C}}

\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\norm}[1]{\lVert#1\rVert}

\renewcommand{\d}{\,\mathrm{d}}
\providecommand{\mtext}[1]{\quad \text{#1} \quad}

\providecommand{\eval}[1]{\left. #1 \right|}
\usepackage[ngerman]{babel} % Deutsches Sprachpaket verwenden
\usepackage[babel,german=quotes]{csquotes}
\usepackage[altbullet]{lucidabr}     % get larger bullet



\begin{document}

% Überschrift
%\begin{center}{\bf \Large Blatt \expage}\end{center}
\section{Multivariate Verteilungen}


Ein $p$-dimensionaler Zufallsvektor ist ein Spaltenvektor
\(X=\begin{pmatrix}X_1\\\vdots\\X_p\end{pmatrix}\).\\
\begin{beispiel}
   Sei $\Omega=\{a,b\}$, $P$ das Wahrscheinlichkeitsmaß auf $\Omega$ mit $P(\{a\})=P(\{b\})=\frac{1}{2}$, $X_1,X_2:\Omega\rightarrow\mathbb{R}$ seien Zufallsvariablen mit:

\[   i=1,2:P(X_i=0)=P(X_i=1)=\frac{1}{2}\] 
      \end{beispiel}

     Dann ist $X=\begin{pmatrix}X_1\\X_2\end{pmatrix}$ ein zweidimensionaler Zufallsvektor. 

     Lässt sich aus den gegebenen Information \(P(X_1+X_2=1)\) berechnen?\\

   Nein, denn die eindimensionale Beschreibung \((1)\) enthält nicht genügend Information über die Verteilung von \(X\).
      \begin{enumerate}
      \item Falls \(X_1(a)=X_2(a)=0\) und \(X_1(b)=X_2(b)=1\), dann gilt \((1)\) und \(P(X_1+X_2=1)=P(\emptyset)=0\).
      \item Falls \(X_1(a)=X_2(b)=1\) und \(X_1(b)=X_2(a)=1\), dann gilt \((1)\) und \(P(X_1+X_2=1)=P(\Omega)=1\).
      \end{enumerate}

Die Verteilung eines Zufallsvektors \(X=\begin{pmatrix}X_1\\\vdots\\X_p\end{pmatrix}\) lässt sich durch seine Verteilungsfunktion \(F=F_X:\mathbb{R}^p\rightarrow\mathbb{R}\) beschreiben, wobei:

\[\begin{pmatrix}x_1\\\vdots\\x_p\end{pmatrix}\in\mathbb{R}^p:F(x_1,\dots,x_p)=P(X_1\le x_1,\dots,X_p\le x_p)\]
\textbf{Beispiel 1.1}   \[F(x_1,x_2)=P(X_1\le x_1, X_2\le x_2)\\X_1\equiv X_2:F(x_1,x_2)=P(X_1\le x_1, X_1\le x_2)=P(X_1\le \min(x_1,x_2))\]
\[=\begin{cases}0&\min(x_1,x_2)<0\\P(X_1=0)=\frac{1}{2}&0\le \min(x_1,x_2)<1\\1&\min(x_1,x_2)\ge 1\end{cases}\]
   Ist \(X=\begin{pmatrix}X_1\\\vdots\\X_p\end{pmatrix}\) ein diskreter Zufallsvektor, d. h. nimmt \(X\) nur endlich oder abzählbar unendlich viele Werte an, dann lässt sich seine Verteilung durch seine Wahrscheinlichkeitsfunktion \(f:\mathbb{R}^p\rightarrow\mathbb{R}\) beschreiben, wobei \(f(x)=P(X=x),x\in\mathbb{R}^p\). Es gilt:
   \[P(X\in D)=\sum_{j:x^{(j)}\in D}f(x^{(j)}),D\subset\mathbb{R}^p\]
   \textbf{Beispiel 1.2}
   \[x^{(1)}=\begin{pmatrix}0\\1\end{pmatrix},x^{(2)}=\begin{pmatrix}1\\0\end{pmatrix}\]
   \[f(x^{(1)})=P(X=x^{(1)})=P(X_1=0,X_2=1)=P(\{a\})=P(\{b\})=f(x^{(2)})=\frac{1}{2}\]
   \[f(x)=0,x\in\mathbb{R}\setminus\{x^{(1)},x^{(2)}\}\]
   Mit $D=\left\{\begin{pmatrix}x_1\\x_2\end{pmatrix}\in\mathbb{R}^2:x_1+x_2=1\right\}$ gilt:
   \[P(X_1+X_2=1)=P(X\in D)=\sum_{j:x^{(j)}\in D}f(x^{(j)})=\frac{1}{2}+\frac{1}{2}=1\]
   Ein Zufallsvektor \(X=\begin{pmatrix}X_1\\\vdots\\X_p\end{pmatrix}\) heißt stetig verteilt, wenn es eine Dichte \(f:\mathbb{R}^p\rightarrow[0,\infty)\) gibt, sodass:
   \[\forall\begin{pmatrix}x_1\\\vdots\\x_p\end{pmatrix}\in\mathbb{R}^p:P(X_1\le x_1,\dots,X_p\le x_p)=\int\limits_{-\infty}^{x_p}\dots\int\limits_{-\infty}^{x_1}f(u_1,\dots,u_p)\d u_1\dots \d u_p\]
\subsection{Mehrdimensionale Integrale}
Zur Berechnung des zweidimensionalen Integrals \(\int\limits_c^d\int\limits_a^b g(x,y)\d x\d y\) berechne zunächst für jedes festgehaltene \(y\in[c,d]\) das innere Integral \(G(y)=\int\limits_a^b g(x,y)\d x\).

   \begin{beispiel}
   Berechne \[\int\limits_0^2\int\limits_0^1 xy \; \d x\d y\]
      \end{beispiel}
   \[g(x,y)=xy\]
   \[G(y)=\int\limits_0^1 xy \; \d x=\left[\frac{x^2y}{2}\right]_{x=0}^1=\frac{y}{2}-\frac{0}{2}=\frac{y}{2}\]
   \[\int\limits_0^2\int\limits_0^1 g(x,y)\, \d x\d y=\int\limits_0^2 G(y)dy=\int\limits_0^2 \frac{y}{2}\d y=\left[\frac{y^2}{4}\right]_{y=0}^2=1\]

Für \(D\subset\mathbb{R}^2\) und \(g:\mathbb{R}^2\to\mathbb{R}\) ist \(\int\limits_D g(x)\d x=\int\limits_D g(x_1,x_2)\,\d x_1 \d x_2\) definiert durch:
\[\int\limits_D g(x)\, \d x=\int\limits_{-\infty}^{\infty}\int\limits_{-\infty}^{\infty} \1_D(x_1,x_2)\cdot g(x_1,x_2)\d x_1 \d x_2 , \1_D(x_1,x_2) \]
\[=\begin{cases}1&(x_1,x_2)\in D\\0&(x_1,x_2)\notin D\end{cases}\]

Im Fall \(D=\mathbb{R}^2\) schreibt man auch \[\int\limits g(x)\d x=\int\limits g(x_1,x_2)\d x_1 \d x_2=\int\limits_{\mathbb{R}^2} g(x)\d x\]
\[=\int\limits_{-\infty}^{\infty}\int\limits_{-\infty}^{\infty} g(x_1,x_2)\d x_1 \d x_2\].\\

Ist \(D\) ein zweidimensionales Intervall der Form \(D=(a_1,b_1)\times(a_2,b_2)\), so ist:
\[\int\limits_{(a_1,b_1)\times(a_2,b_2)}g(x)\d x=\int\limits_{-\infty}^{\infty}\int\limits_{-\infty}^{\infty} 1_D(x_1,x_2)\cdot g(x_1,x_2)\;\d x_1 \d x_2\]
\[=\int\limits_{a_2}^{b_2}\int\limits_{a_1}^{b_1} \1_D(x_1,x_2)\cdot g(x_1,x_2) \d x_1 \d x_2 = \int\limits_{a_2}^{b_2} \int\limits_{a_1}^{b_1} g(x_1,x_2) \d x_1 \d x_2 \]

   \subsection{Rechenregeln}
   \begin{itemize}
   
   \item
   Linearität
   \[\forall \; \alpha,\beta\in\mathbb{R},\;f,g:\mathbb{R}^2\to\mathbb{R},\,D\subset\mathbb{R}^2: \int\limits_D (\alpha\cdot f(x)+\beta\cdot g(x)\d x)=\alpha\int\limits_D f(x)\d x+\beta\int\limits_D g(x)\d x\]
   
   \item
   Additivität bezüglich des Integrationsbereichs\\
   Für disjunkte Mengen \(D_1 , D_2\subset\mathbb{R}^2, D_1 \cap D_2 =\emptyset\) gilt:
   \[\int\limits_{D_1 \cup D_2} g(x) \d x = \int\limits_{D_1} g(x)\d x + \int\limits_{D_2} g(x)\d x\]
   
   \end{itemize}

% \section*{16.10.2013}
\subsection{Monotonie}
$$\int\limits_D f(x) \mathrm{d}x \leq \int\limits_D g(x) \d x \quad \text{falls} \quad f(x) \leq g(x), \, \forall x \in D$$

$$ \int\limits_{D_1} g(x) \d x \leq \int\limits_{D_2} g(x) \d x \mtext{falls} D_1 \subset D_2 \subset \R^2 \mtext{und} g(x) \geq 0, \, \forall x \in D_2$$

\subsection{Vertauschbarkeit der Integrationsreihenfolge}

\[\int\limits_{a_2}^{b_2}  \int\limits_{a_1}^{b_1}  g(x_1, x_2) \d x_1 \d x_2   =  \int\limits_{a_1}^{b_1} \int\limits_{a_2}^{b_2} g(x_1, x_2) \d x_2  \d x_1, \, -\infty \leq a_1 \leq b_1 \leq \infty , -\infty \leq a_2 \leq b_2 \leq \infty\]

Für Integranden der Form $g(x_1, x_2) = h_1(x_1, x_2) h_2(x_2)$ gilt:

\[ \int\limits_{a_2}^{b_2} \int\limits_{a_1}^{b_1}  h_1(x_1, x_2) h_2(x_2) \d x_1 \d x_2  = \int\limits_{a_2}^{b_2} h_2(x_2) \int\limits_{a_1}^{b_1} h_1(x_1, x_2) \d x_1 \d x_2 \]

Für Integranden der Form $ h_1(x_1) h_2(x_2) $ gilt:

\[ \int\limits_{a_2}^{b_2} \int\limits_{a_1}^{b_1} h_1(x_1) h_2(x_2) \d x_1 \d x_2 = \int\limits_{a_2}^{b_2} h_2(x_2) \int\limits_{a_1}^{b_1} h_1(x_1) \d x_1 \d x_2 =  \int\limits_{a_1}^{b_1} h_1(x_1) \d x_1 \int\limits_{a_2}^{b_2} h_2(x_2) \d x_2 \]

Analoge Regeln und Bezeichnungen gelten für höherdimensionale Integrale. Zum Beispiel gilt für $g:\R^3 \to \R$ und $a_i \leq b_i, i=1,2,3,\ldots$

\[ \int\limits_{a_1}^{b_1} \int\limits_{a_2}^{b_2} \int\limits_{a_3}^{b_3}  g(x_1, x_2, x_3) \d x_1 \d x_2 \d x_3 =  \int\limits_{a_2}^{b_2}  \int\limits_{a_3}^{b_3} \int\limits_{a_1}^{b_1} 
g(x_1, x_2, x_3) \d x_2 \d x_3 \d x_1\]

Die Integration kann in beliebiger Reihenfolge durchgeführt werden.

Für $ h_1, \ldots, h_p: \R^1 \to \R^1 $ gilt:

\[ \int\limits_{a_p}^{b_p} \cdots \int\limits_{a_1}^{b_1}   h_1(x_1) \cdots h_p(x_p) \d x_1 \cdots \d x_p = \int\limits_{a_1}^{b_1} h_1(x_1) \d x_1 \cdots \int\limits_{a_p}^{b_p} h_p(x_p) \d x_p \]

In Beispiel 2:

\[ \int\limits_0^2 \int\limits_0^1 x y\, \d x \d y =  \int\limits_0^1 x  \d x \cdot \int\limits_0^2 y \d y = \eval{\frac{x^2}{2}}_0^1 \eval{\frac{y^2}{2}}_0^2  = \frac{1}{2} \cdot 2 = 1 \]

Ist $X = (X_1, \ldots, X_p)^T $ ein Zufallsvektor mit Dichte $f: \R^p \to \R$, dann gilt:

\[ P(X \in D) = \int\limits_D f(x) \d x \mtext{für} D \subset R^p \mtext{und} \int f(x) \d x = 1 \]

\begin{beispiel}: Sei $X = (X_1, X_2)^T$ Zufallsvektor mit Dichte

\[
f(x_1, x_2) = \begin{cases}
1 \mtext{falls} 0 \leq x_1 \leq 1, \, 0 \leq x_2 \leq 1 \\ 0 \mtext{sonst} \end{cases}\] also Gleichverteilung auf $[0,1]^2$.
\end{beispiel}

Berechne $P(X_1 \leq X_2)$.

Setze 
\[D := \{ (X_1, X_2)^T \in R^2: x_1 \leq x_2 \}\] Daher gilt 
\[ P(X_1 \leq X_2) = P(X \in D) = \int\limits_D f(x) \d x = \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty}  \1_D (x_1, x_2) f(x_1, x_2) \d x_1 \d x_2\] ($\1$ bezeichnet die Indikatorfunktion).

\[ \int\limits_0^1 \int\limits_0^1 \1_D (x_1, x_2) f(x_1, x_2) \d x_1 \d x_2 = \int_0^1 x_2 dx_2 = \eval{\frac{x_2^2}{2}}_0^1 = \frac{1}{2} \] Die Indikatorfunktion kann nur die Werte 1 oder 0 annehmen und ist gleich $1$, falls $0 \leq x_1 \leq x_2 \leq 1$ und gleich $0$ sonst.

Sei $ X $ ein $p$-dimensionaler Zufallsvektor mit Dichte $ f:\R^p \to \R  $. Betrachte den partitionierten Vektor $ X= (x_1, x_2)^T $ wobei $X_1$ $k$-dimensional und $X_2$ $p-k$-dimensionale Zufallsvektoren sind, $1 \leq k \leq p-1$.

$X_1$ hat die sogenannte \textbf{Randdichte} \[f_{X_1}(x_1) = \int f(x_1, x_2) \d x_2,\, x_1 \in R^k . \]

% \section{21.10.2013}

$X$ p-dimensional. Dichte $f$. \[ X = (X_1, X_2)^T ,\] $ X_1 $ k-dimensional, $X_2$ p-k-dimensional. $X_1$ hat die Randdichte:

\[ f_{X_1}(x_1) = \int f(x_1, x_y) \d x_2, \, x_1 \in \R^k .\]

$X_2$ hat die Randdichte \[ f_{X_2} (x_2) = \int f(x_1, x_2) \d x_1, \, x_2 \in \R^{p-k}. \]


$f$ heisst dann auch \emph{gemeinsame Dichte}. Die Verteilungen von $X_1$ und $X_2$ heißen \emph{Randverteilungen} (\emph{Marginalverteilungen}).\\




\begin{beweis}
$f_{X_1}$ ist eine Dichte von $X_1$, für den Fall $p=2$, $k=1$. Für jedes $x_1 \in \R$ gilt:

\[ P(X_1 \leq x_1) = P(X_1 \leq x_1, \, X_2 \in \R) = P( X \in (- \infty, x_1 ] \times \R) = \int\limits_{(-\infty, x_1] \times \R} f(u) \d u \]
\[= \int\limits_{-\infty}^{x_1} \int\limits_{-\infty}^{\infty} f(u_1, u_2) \d u_2 \d u_1  =  \int\limits_{-\infty}^{x_1} f_{X_1} (u_1)\d u_1 \]

\end{beweis}
	
\begin{beispiel}
$ X = (X_1, X_2)^T $ habe die Dichte $f$ aus Beispiel 3.
$ Y = (Y_1, Y_2)^T$ habe die Dichte $g(y_1, y_2) = \begin{cases} 1+(2 y_1 - 1)(2y_2 - 1), \, (y_1, y_2) \in v \in [0,1]^2 \\ 0 \mtext{sonst}.\end{cases}$

Bestimme Randdichten $f_{X_i}, g_{y_i}$

\end{beispiel}

\[ f_{X_1}(x_1) = \int\limits_{-\infty}^{\infty}f(x_1, x_2) \d x_2 = 
\begin{cases} 1, \, x_1 \in [0,1] \\
0 \, \mtext{sonst.} \end{cases} \]

\[ f_{X_2}(x_2) = 
\begin{cases} 1, \, x_1 \in [0,1] \\
0 \, \mtext{sonst.} \end{cases} \]


\[ g_{y_1} (y_1) = \int\limits_{-\infty}^{\infty} g(y_1, y_2) \d y_2  \]

\[= \begin{cases} \int\limits_0^1 1+(2 y_1 - 1)(2y_2 - 1) \d y_2 = 1+ (2y_1 - 1) [y_2^2 - y_2]_0^1 = 1, \, y_1 \in [0,1]  \\ 0 \mtext{sonst} \end{cases} \]

analog folgt 
\[ g_{y_2}(y_2) = 
\begin{cases} 1, \, y_2 \in [0,1] \\
0 \, \mtext{sonst.} \end{cases} \]

$\rightarrow X_1, X_2, Y_1, Y_2$ haben dieselben Randverteilungen, aber $X$ und $Y$ haben verschiedene Verteilungen.



Sei wieder $X = (X_1, X_2)^T$ p-dimensionaler Zufallsvektor mit Dichte $f: \R^p \to \R$. $X_1$ $k$-dimensionaler und $X_2$ $p-k$-dimensionaler Zufallsvektor. Beschreibe die Verteilung von $X_1$ unter der Annahme, dass $X_2$ den festgehaltenen Wert $x_2$ annimmt - $X$ bezeichnet hier eine Zufallsvariable, $x$ ist deterministisch.

Für $x_2 \in \R^{p-k}$ ist die \emph{bedingte Dichte} von $X_1$ gegeben $X_2 = x_2$ definiert durch 

\[ f_{X_1}(x_1 \, | \,  X_2 = x_2) = \frac{f(x_1, x_2)}{f_{X_2} (x_2) } , \, x_1 \in \R^k, \, \mtext{falls} \, f_{X_2}(x_2) > 0. \]

Die bedingte Dichte von $X_2$ gegeben $X_1 = x_1$ ist

\[ f_{X_2}(x_2 \, | \, X_1 = x_1) = \frac{f(x_1, x_2)}{f_{X_1} (x_1) } , \, x_2 \in \R^p-k, \, \mtext{falls} \, f_{X_1}(x_1) > 0. \]

$X_1$ und $X_2$ heißen \emph{(stochastisch) unabhängig}, falls 

\[ f(x_1, x_2) = f_{X_1}(x_1) f_{X_2}(x_2), \, \forall x_1 \in \R^k, \,  x_2 \in \R^{p-k}. \]

Falls $X_1$ und $X_2$ unabhängig sind, gilt für alle möglichen Werte die hier miteinander verglichen werden können, also für alle $x_1 \in \R^k$ mit der Eigenschaft, dass die bedingte Dichte definiert ist, also $f_{X_1}(x_1) > 0$:

\[ f_{X_2}(x_2 \, | \,  X_1 = x_1) = \frac{f(x_1, x_2)}{f_{X_1} (x_1)} =  \frac{f_{X_1}(x_1) f_{X_2} (x_2)}{f_{X_1} (x_1)} = f_{X_2}(x_2) \]

Das heisst die bedingte Dichte von $X_2$ gegeben $X_1 = x_1$ hängt nicht von $x_1$ ab und ist gleich der Randdichte von $X_2$.

Sind $X_1, \ldots, X_p$ Zufallsvariablen und $f$ eine Dichte von $(X_1, \ldots, X_p)^T$, dann sind $(X_1, \ldots, X_p)$ unabhängig, falls

\[ f(x_1, \ldots, x_p) = f_{X_1}(x_1) \cdots f_{X_p}(x_p), \, \forall x_1, \ldots, x_p \in \R. \]

\begin{beispiel}

$ X = (X_1, X_2)^T $ sei Zufallsvektor mit Dichte 
\[ f(x_1, x_2) = \begin{cases} x_1 + x_2, \, \mtext{für} \, 0 \leq x_1, x_2 \leq 1  \\ 0, \, \mtext{sonst}. \end{cases}  \] 
Randdichte:

\end{beispiel}

\[ f_{X_1}(x_1) = \begin{cases} \int\limits_{0}^1 x_1+ x_2 \d x_2 = x_1 + [\frac{x_2^2}{2}]_0^1 = x_1 + \frac{1}{2}, \, x_1 \in [0,1] \\ 0, \ ,\mtext{sonst.} \end{cases} \]

\[ f_{X_2}(x_2) = \begin{cases} x_2 + \frac{1}{2}, \, x_2 \in [0,1] \\ 0, \ ,\mtext{sonst.} \end{cases} \]
Es gilt also nicht $f(x_1, x_2) = f_{X_1}(x_1) f_{X_2}(x_2) \forall x_1, x_2 \in \R .$ das heisst $X_1$ und $X_2$ sind nicht unabängig.

Bedingte Dichten: Für $ x_2 \in [0,1]$ ist \[ f_{X_1}(x_1 \, | \,  X_2 = x_2) = \frac{f(x_1, x_2)}{f_{X_2}(x_2)} = \begin{cases} \frac{x_1 + x_2}{0.5 + x_2}, \, x_1 \in [0,1] \\0 \, \mtext{sonst.}  \end{cases} \]


Für $ x_1 \in [0,1]$ ist \[ f_{X_2}(x_2 \, | \, X_1 = x_1) = \begin{cases} \frac{x_1 + x_2}{x_1 + \frac{1}{2}}, \, x_2 \in [0,1] \\ 0, \, \mtext{sonst.} \end{cases} \]

Für $0 \leq \alpha \leq \beta \leq 1$ und für $0<\delta \leq \frac{1}{2}$ gilt:

\[ P (\alpha \leq X_1 \leq \beta\, | \, \frac{1}{2} \leq X_2 \leq \frac{1}{2 + \delta} )  = \frac{ P( \alpha \leq X_1 \leq \beta, \, \frac{1}{2} \leq X_2 \leq \frac{1}{2} + \delta )}{P(0.5 \leq X_2 \leq 0.5 + \delta)}  = \frac{\int\limits_\alpha^\beta \int\limits_{\frac{1}{2}}^{\frac{1}{2} + \delta} f(x_1, x_2) \d x_2 \d x_1}  {  \int\limits_{\frac{1}{2}}^{\frac{1}{2} + \delta} f_{X_2} (x_2) \d x_2 }  \]  

\[ = \frac{\int\limits_\alpha^\beta \int\limits_{\frac{1}{2}}^{\frac{1}{2} + \delta} x_1 + x_2 \d x_2 \d x_1}  {  \int\limits_{\frac{1}{2}}^{\frac{1}{2} + \delta} 0.5 + x_2  \d x_2 } =   \frac{\int\limits_{\alpha}^{\beta} \delta x_1 + [\frac{x_2^2}{2}]_{x_2 = 0.5}^{0.5 + \delta} \d x_1}  { \frac{\delta}{2} + [\frac{x_2^2}{2}]_{0.5}^{0.5 + \delta} } \]

\[ = \frac{ \int\limits_\alpha^{\beta} \delta x_1 + \frac{\delta}{2} + \frac{\delta^2}{2} \d x_1} {\delta + \frac{\delta^2}{2}} =  \frac{ \int\limits_\alpha^\beta x_1 + \frac{1}{2} \d x_1 + \frac{\delta}{2}  (\beta - \alpha)} {1 + \frac{\delta}{2}}\]

 \[\rightarrow  \lim_{\delta \to 0^{+}}
  P(\alpha \leq X_1 \leq \beta\, | \, 0.5 \leq X_2 \leq 0.5 + \delta) = \int\limits_{\alpha}^{\beta} x_1 + 0.5 \d x_1 = \int\limits_{\alpha}^\beta f_{X_1}(x_1 | X_2 = 0.5) \d x_1  \quad \blacksquare\]

Wichtige Parameter für univariate Verteilungen: Erwartungswert und Varianz. Entsprechende Parameter für multivariate Verteilungen ist der Vektor der Erwartungswerte und Kovarianzmatrix. Sei $X = (X_1, \ldots X_p)^T$ $p$-dimensionaler Zufallsvektor mit Dichte $f$. 

\[  E(X_1) = \int\limits_{-\infty}^{\infty} x_1 f_{X_1}(x_1) \d x_1  = \int\limits_{-\infty}^{\infty} x_1   \int\limits_{-\infty}^{\infty} \cdots \int\limits_{-\infty}^{\infty} f(x_1, \ldots, x_p) \d x_2 \ldots \d x_p \d x_1 =  \int x_1 f(X) \d x.  \]

Entsprechend ist der Erwartungswert von $X_j$

\[  E(X_j) =  \int\limits_{-\infty}^{\infty} x_j f_{X_j} \d x_j = \int x_j f(x_j) \d x, \, j = 1, \ldots , p \]


Erwartungswert(-vektor) ist definiert durch $E(X) = (E(X_1), \ldots, E(X_p))^T$.


Für $g \cdot \R^p \to \R$ ist $E[g(X)] = \int g(x) f(x) \d x.$ Hierbei sind die Komponenten Vektoren, also:

\[  \int\limits_{-\infty}^{\infty} \cdots \int\limits_{-\infty}^{\infty} g(x_1, \ldots, x_p) f(x_1, \ldots, x_p) \d x_1 \ldots \d x_p . \]

Allgemeiner: Für matrixwertige Funktionen $G: \R^p \to \R^{n\times m}$:

\[   G(x) = \begin{pmatrix}

g_{11}(x)  & \ldots & g_{1m}(x) \\
\vdots & & \vdots \\
g_{n1}(x) & \ldots & g_{nm}(x) \\


\end{pmatrix}  \]
ist $E[G(x)]$ komponentenweise definiert.



% \section{23.10.13}


\[ E[G(X)] =  \begin{pmatrix}

E[g_{11}(X)]  & \ldots & E[g_{1m}(X)] \\
\vdots & & \vdots \\
E[g_{n1}(X)] & \ldots & E[g_{nm}(X)] \\

\end{pmatrix}  =  (E[g_{ij}(X)])_{i = 1, \dots, n, \; j = 1,\dots, m} \]

Für matrixwertige Funktion $  H: \R^p \to \R^{n\times m}, \, H(x) = (h_{ij}(x))_{i,j} $ ist 
$ \int H(x) \d x  $ ebenfalls komponentenweise definiert.

\[ \int H(x) \d x =   \begin{pmatrix}

\int h_{11}(x) \d x]  & \ldots & \int h_{1m}(x) \d x \\
\vdots & & \vdots \\
\int h_{n1}(x) \d x & \ldots & \int h_{nm}(x) \d x \\

\end{pmatrix}  \]

Damit ergibt ($G$ matrixwertig) sich \[ E[G(X)] = E[g_{ij}(X)]_{i,j} = \left(\int g_{ij}(x)f(x)\d x \right)_{i,j} = \int G(x)f(x) \d x \]
und insbesondere \[  E(X) = \int x f(x) \d x   \]

\begin{beispiel}
Berechne $E(X)$ für $X=(X_1, X_2)^T$ aus Beispiel 5.
\end{beispiel}
\[ E(X_1) = \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty} x_1 f(x_1, x_2) \d x_1 \d x_2
 =   \int\limits_{0}^{1} \int\limits_{0}^{1} x_1 (x_1 + x_2) \d x_1 \d x_2  \]
 

 \[=    \int\limits_{0}^{1} x_1^2 \d x_1   + x_2 \int\limits_0^1 x_1 \d x_1 \d x_2
= \int\limits_0^1 \frac{1}{3} + x_2 \, \frac{1}{2} \d x_2 = \frac{1}{3} + \frac{1}{2} \, \frac{1}{2} = \frac{7}{12}\]

Daher \[  E(X_2) = \cdots = \frac{7}{12}    \]
\[ E(X) = (E(X_1), E(X_2))^T = (\frac{7}{12}, \frac{7}{12})^T   . \]

\subsection{Rechenregeln}

Für $p$-dimensionale Zufallsvektoren $X,Y$, $\alpha, \beta \in \R$, $g,h: \R^p \to \R^n$ gilt:

\[  E[\alpha g(X) + \beta  h(Y)   ]  = \alpha E[g(X)] + \beta E[h(Y)]   \]
\[  E[AX + b] = AE(X) + b, \mtext{wobei} A \in \R^{n \times p}, \, b \in \R^n   \]


Ist $X = (X_1, X_2)^T$ mit $X_1$ $k$-dimensional und $X_2$ $p-k$-dimensional und ist $g:\R^k \to \R$, dann 
\[ E[g(X_1)]  = \int g(X_1) f(x) \d x = \int g(X_1)  f_{X_1} (x_1) \d x_1. \]

Sind $X_1$ und $X_2$ unabhängig und $g:\R^k \to \R, \; h:\R^{p-k} \to \R$, dann
\[ E[g(X_1) h(X_2)] = E[g(X_1)]E[h(X_2)]    \]

Nachweis für den Fall $p=2, \, k = 1$:

\[ E[g(X_1) h(X_2)] =  \int \int g(x_1)h(x_2) f(x_1, x_2) \d x_1 \d x_2 \]
\[\int \int g(x_1)h(x_2) f_{X_1} (x_1) f_{X_2}(x_2) \d x_1  \d x_2    = \int g(x_1) f_{X_1} (x_1) \d x_1  \int  h(x_2) f_{X_2} (x_2) \d x_2\]
\[ = E[g(X_1)] E[h(X_2)] \blacksquare  \]


Sind $X,Y$ $p$-dimensionale unabhängige Zufallsvektoren, dann gilt 
\[  E(XY^T) = E(X) E(Y)^T \mtext{und} E(X^T Y) = E(X)^T E(Y)    \]

denn mit $X = (X_1, \ldots, X_p)^T, \, Y=(Y_1, \ldots, Y_p)^T $ ist

\[  E(XY^T) =     \begin{pmatrix}

(X_1)  &  & &  \\
\vdots & (Y_1 & \ldots & Y_n) \\
(X_p) &  & & \\

\end{pmatrix}  = (E(X_jY_k))_{j,k = 1,\ldots,p}  = (E(X_j)E(Y_k))_{j,k = 1, \ldots, p}   \]

\[ \begin{pmatrix}

E(X_1)   \\
\vdots \\
E(X_p) \\

\end{pmatrix} (E(X_1), \ldots, E(Y_p)) = E(X) E(Y)^T  \]

\[ E(X^T Y) = E(\sum_{j=1}^{p} X_j Y_j) = \sum_{j=1}^{p} E(X_j) E(Y_j) = E(X)^T E(Y) \blacksquare  \]


Einschub für Übungsblatt, Kovarianz:
\[  \Cov(X_j, X_k) := E[X_j - E(X_j) (X_k - E(X_k))]  \]




% \section{28.10.13}

Sei $ X = (X_1, \dots, X_p)^T $ p-dimensionaler Zufallsvektor und $ \mu = (\mu_1, \dots, \mu_p) = E(X) $. (Alle $p$s unten sind eigentlich müs $\mu$)

Varianz von $X_j$:

\[  \sigma_j^2 = Var(X_j) = E[(X_j - \mu_j)^2]     \] ist ein Maß für die Streuung von $X_j$.

Kovarianz zwischen $X_j$ und $X_k$ 

\[  \sigma_{jk}^2 = \Cov(X_j, X_k) = \E[(X_j - \mu_j)(X_k - \mu_k )]   \]

ist ein Maß für den Zusammenhang zwischen $X_j$ und $X_k$, $j \neq k$, $(\sigma_{jj} = \sigma_j^2)$.

\[ \Cov (X_j, X_k) \geq 0:   \] $X_j$ und $X_k$ weisen tendenziell einen gleichsinnigen linearen Zusammenhang auf.

\[ \Cov (X_j, X_k) \leq 0:  \] Sind $X_j$ und $X_k$ unabhängig gilt:
\[  \Cov (X_j, X_k) = \E(X_j - \mu_j) \E(X_k - \mu_k)   = 0. \]

Aus $\Cov(X_j, X_k) = 0$ folgt im allg. nicht, dass $X_j$ und $X_k$ unabhängig sind, siehe Aufgabe 2, Blatt 1.

Die \textbf{Kovarianzmatrix} $\Sigma$ der Zufallsvariable $X$ ist definiert durch 

\[ \Sigma = Var(X) =  \E[(X-\mu)(X - \mu)^T] \]

\[ \Sigma =   (\E[(X_j - \mu_j)(X_k - \mu_k)^T])_{j,k = 1,\ldots,p} \]

\[= \begin{pmatrix}

\sigma_1^2 & \sigma_{12} & \ldots & \sigma_{1p} \\
\sigma_{21} & \sigma_2^2 & \ldots & \sigma_{2p} \\
\vdots & & \ddots  & \\
\sigma_{p1} && \ldots & \sigma_{p}^2 \\

\end{pmatrix}  \]


Allgemeiner: Ist $X = (X_1, \ldots, X_p)^T$ ein $p$-dimensionaler und $Y = (Y_1, \ldots, Y_q)^T$ ein $q$-dimensionaler Zufallsvektor dann heißt

\[ \Cov(X,Y) = \E[(X-\E(x))(Y-\E(Y))^T  ]  \] die Kovarianzmatrix von $X$ und $Y$. In Position $(j,k)$ von $ \Cov(X,Y) $ steht

\[ \E [(X_j - \E(X_j))(Y_k - \E(Y_k)) ] = \Cov(X_j, X_k) \] also \[ \Cov(X,Y) = (\Cov(X_j, Y_k))_{j = 1, \ldots, p, \; k = 1, \ldots, q} \]

\begin{lemma} Sei X ein $p$-dimensionaler, $Y$ ein $q$-dimensionanler Zufallsvektor.\\ \end{lemma}
\begin{enumerate}
\item $\Cov(X,X) = \Var(X)$\\
\item $\Cov(X,Y) = \E(XY^T) - \E(X)[\E(Y)]^T$, $\Var(X) = \E(XX^T)-\E(X)(\E(X))^T $\\
\item $\Var(a^T X) = a^T \Var(X) a$ für $a \in \R^p$\\
\item $\Var(AX + b) = A \Var(X) A^T$ für $a\in \R^{n\times p}$, $b \in \R^n$\\
\item Sei $p = q$. Dann gilt $\Var(X+Y) = \Var(X) + \Var(Y) + \Cov(X,Y) + \Cov(Y,X)$. Falls $X$ und $Y$ unabhängig sind, dann $\Cov(X,Y) = \Cov(Y,X) = 0$ und $\Var(X+Y) = \Var(X) + \Var(Y)$.\\
\item $\Var(X)$ ist symmetrisch und nicht-negativ definit, d.h. $a^T \Var(X)a \geq 0, \; \forall a\in \R^p$\\
\end{enumerate}

\begin{beweis} Setze $\mu$ = $\E(X)$, $\upsilon = \E(Y)$ \end{beweis}
\begin{enumerate} %add stuff for letters in enumerate
\item $\Cov (X,Y) = \E[(X-\mu)(X-\mu)^T] = \Var(X)$\\
\item $\Cov(X,Y) = \E[(X-\mu)(Y-\upsilon)^T] = \E(XY^T - \mu Y^T - X\upsilon^T + \mu \upsilon^T) = \E(XY^T) - \mu \E(Y^T) - \E(X)\upsilon^T + \mu \upsilon^T = \E(XY^T - \E(X)(\E(Y))^T$. Daraus folgt mit (a), dass $\Var(X) = \Cov(X,X) = \E(XX^T) - \E(X)(\E(X))^T $\\
\item Spezialfall von (d) mit $A = a^T, \; b = 0$\\
\item $\Var(AX + b) = \E[(AX + b - \E(AX + b)) (AX + b - \E(AX + b))^T ] = A \E[(X-\E(X)) (X-\E(X))^T A^T ] = A \Var(X) A^T $\\

\item $\Var(X + Y) = \E[(X + Y)(X+Y)]^T - (\E(X+Y))^T = \E(XX^T) + \E(XY^T) + \E(YX^T) + \E(YY^T) - \E(X)(\E(X))^T - \E(X)(\E(Y))^T - \E(Y)(\E(X))^T - \E(Y)(\E(Y))^T = \Var(X) + \Cov(X,Y) + \Cov(Y,X) + \Var(Y). $ Sind $X = (X_1, \ldots, X_p)^T$ und $Y = (Y_1, \ldots, Y_p)^T$ unabhängig, dann gilt $\E(X_j, Y_k) = \E(X_j)\E(Y_k)$ also $\Cov(X_j, Y_k) = 0$. Daraus folgt $\Cov(X,Y) = 0, \; \Cov(Y,X) = 0.$\\
\item $ \{ \Var(X) \}_{jk} = \Cov(X_j, X_k) = \Cov(X_k, X_j) = \{ \Var(X) \}_{kj}$ für alle $j,k = 1, \ldots, p$. D.h. $\Var(X)$ ist symmetrisch.
Für alle Vektoren $a^T \in \R^p$ gilt $a^T \Var(X)a = \Var(a^T X) \geq 0$ (wegen c).\\

\end{enumerate}

\begin{beispiel} Sei $X = (X_1, X_2)$ wie in Bsp. 5. Berechne (i) $\Var(X)$, (ii) $\Var(X_1 - 3X_2)$, (iii) $\Var(X_1 + X_2 + 1, 2X_1 - X_2 - 1)^T$.
\end{beispiel}

\begin{enumerate}
\item $\E(X_1) = \E(X_2) = \frac{7}{12}$ nach Bsp. 6. \\ 
\[\E(X_1^2) = \int x_1^2 f_{X_1}(x_1) \d x = \int\limits_0^1 x_1^2 (x_1 + \frac{1}{2}) \d x = \eval{\frac{x_1^4}{4}}_0^1 + \eval{\frac{x_1^3}{6}}_0^1
= \frac{1}{4} + \frac{1}{6} = \frac{5}{12} \]

\[\implies \Var(X_1) = \E(X_1^2 - (\E(X_1))^T = \frac{5}{12} - (\frac{5}{12})^T = \frac{11}{144} \mtext{ebenso} \Var{X_2} = \frac{11}{144}.\]

\[\E(X_1 X_2) = \iint x_1 x_2 f(x_1, x_2) \d x_1 \d x_2 = \int\limits_0^1 \int\limits_0^1 x_1 x_2 f(x_1, x_2) \d x_1 \d x_2 \]
\[= \int\limits_0^1 x_1^2 \d x \int\limits_0^1 x_2 \d x_2 + \int\limits_0^1 x_1 \d x_1 \int\limits_0^1 x_2^2 \d x_2 = \frac{1}{3} \frac{1}{2} + \frac{1}{2} \frac{1}{3} = \frac{1}{3}\]

\[\Cov(X_1, X_2) = \E(X_1 X_2) - \E(X_1) \E(X_2) = \frac{1}{3} - \frac{7}{12} \cdot \frac{7}{12} = -\frac{1}{144} \implies \Var(X) \]

\[ \begin{pmatrix}
\Var(X_1) & \Cov(X_1, X_2) \\
\Cov(X_2, X_1) & \Var(X_2) \\
\end{pmatrix}  =
\begin{pmatrix}
\frac{11}{144} & -\frac{11}{144}  \\
-\frac{11}{144} & \frac{11}{144} \\
\end{pmatrix} 
 \]\\

 \item Mit $a = (1, -3)^T$ ist $X_1 - 3X_3 = a^T X \implies \Var(X_1 - 3 X_3) =\Var(a^T X) = a^T \Var (X) a $. 

 Einsetzen liefert $\frac{29}{36}$.\\

 \item Mit $A = \begin{pmatrix} 1 & 1 \\ 2 & - 1 \\ \end{pmatrix}$ und $ b = (1, -1)^T$ ist $AX + b = \begin{pmatrix} X_1 + X_2 + 1 \\ 2X_1 - X_2 - 1 \end{pmatrix}  \implies \Var(AX + b) = A \Var(X) A^T$\\

\end{enumerate}

\newpage


\subsection*{Übungsblatt 1}

\begin{aufgabe} 
\end{aufgabe}

a)

\[  f_{X_1}(x_1) = \int\limits_0^1 f(x_1, x_2) \d x_2 = 4x_1 e^{-x_1^2} \int\limits_0^1 x_2 \d x_2 = x_1 e^{-x_1^2}, \quad x_1 \geq 0.     \]

\[  f_{X_1}x_1 = 0, \quad x_1 < 0     \]

b)

\[  f_{X_1}(x_2) = \int\limits_0^{\infty}f(x_1, x_2) \d x_1 = 4 x_2 \int\limits_0^{\infty} x_1 e^{-x_1^2} \d x_1  = \eval{-\frac{e^{-x_1^2}}{2}}_0^\infty  = 2x_2, \quad x_2 \in [0,1]  \]

\[   \implies f_{X_1}(x_1 | X_2 = \frac{1}{2}) = \frac{f(x_1, \frac{1}{2})}{f_{X_2}(\frac{1}{2})} = 2x_1 e^{-x_1^2}, \quad x_1 \geq 0, x_1 < 0.    \]

c)

\[  \E(X_1) = \int\limits_0^{\infty} x_1 f_{X_1}(x_1) \d x_1 \underbrace{=}_{a)} 2 \int\limits_0^\infty x_1^2 e^{-x_1^2} \d x_1    \]
\[  = \int\limits_{-\infty}^{\infty}  x_1^2 e^{-x_1^2} \d x_1 \underbrace{=}_{x_1 = \frac{t^2}{2}}    \int\limits_{-\infty}^{\infty}  \frac{t^2}{2} e^{-\frac{t^2}{2}}  \frac{1}{\sqrt{2}} \d t =      \frac{\sqrt{\pi}}{2}   \int\limits_{-\infty}^{\infty} t^2 \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}} \d t = \frac{\sqrt{\pi}}{2}    \]

\[    \E(X_2) =  \int\limits_0^1 x_2 f_{X_2} (x_2) \d x_2 \underbrace{=}_{b)} \int\limits_0^1 2 x_2^2 \d x_2 = \frac{2}{3} \implies \E(X) = \left(\frac{\sqrt{\pi}}{2}, \frac{2}{3}\right)^T     \]

d)
Nach a), b) gilt 
\[  f_{X_1} f_{X_2} = f(x_1, x_2) \; \forall x_1, x_2 \in \R \quad \implies X_1, X_2 \mtext{sind unabhängig.} \Cov(X_1, X_2) = 0.      \]

e)
\[   P(X_1 > X_2) = \int\limits_0^1 \int\limits_{x_2}^\infty f(x_1, x_2) \d x_1  \d x_2  = \int\limits_0^1 4x_2 \int\limits_{x_2}^\infty x_1 e^{-x_1^2} \d x_1 \d x_2    \]
\[  = \int\limits_0^1 4x_2 \eval{ -\frac{e^{-x_1^2}}{2}  }_{x_1 = x_2}^{\infty} \d x_2  = \int_0^1 4x_2 \frac{e^{-x_2^2}}{2} \d x_2 = \eval{-e^{-x_2^2}}_0^1 = 1-e^{-1}    \]\\


\begin{aufgabe} 
\end{aufgabe}
a)

\[  1 = c \int\limits_-1^1  \int\limits_-1^1 x_1^2 x_2^2 \d x_1 \d x_2 = \int\limits_-1^1  \frac{2}{3} + 2 x_2^2 \d x_2   \] 
\[  = c( \frac{4}{3} + \frac{4}{3} ) = \frac{8c}{3}  \implies c = \frac{3}{8}    \]

b)

Für $x_1 \in [-1, 1]$ gilt

\[   f_{X_1}(x_1) = \int\limits_{-1}^1 f(x_1, x_2) \d x_2  = c \int_{-1}^1 x_1^2 + x_2^2 \d x_2 = c(2x_1^2 + \frac{2}{3})  \]
\[   f_{X_2}(x_2 | X_1 = x_1) = \frac{f(x_1, x_2)}{f_{X_1} (x_1) } = \frac{c(x_1^2 + x_2^2 ) }{c(2x_1^2 + \frac{2}{3})}  = \frac{x_1^2 + x_2^2}{2x_1 + \frac{2}{3}} \mtext{falls} x_2 \in [-1, 1]  \]

\[   f_{X_2}(x_2|X_1 = x_1) = 0 \mtext{für} x_2 \in \R \setminus [-1, 1]    \]

c)

\[  \E(X_1) = \int\limits_{-1}^1 x_1 f_{X_1}(x_1)\d x_1 \underbrace{=}_{b)} = c\int_{-1}^1 2x_1^3 + \frac{2x_1}{3} \d x_1  = 0 \quad \E(X_2) = 0 \mtext{analog.}    \]
\[   \Cov(X_1, X_2) = \E(X_1 X_2) = c \int\limits_{-1}^1 \int\limits_{-1}^1  x_1 x_2 (x_1^2 + x_2^2) \d x_1 \d x_2   \]
\[   = c \int\limits_{-1}^1 \int\limits_{-1}^1 \underbrace{x_1^3 x_2 + x_1 x_2^3}_{\mtext{ungerade Funktion von} x_1} \d x_1 \d x_2  = 0  \]

d) $X_1$ und $X_2$ sind nicht unabhängig, denn nach b) ist die bedingte Dichte der zweiten Variable $f_{X_2}(x_2 | X_1 = x_1)$ nicht unabhängig von $x_1$.\\


\begin{aufgabe} 
\end{aufgabe}

Für $j = 1, \ldots, p$ und alle $x_j \in \R$ gilt:

\[  f_{X_j} (x_j) = \int \cdots \int f (x_1, \ldots x_p) \d x_1 \ldots \d x_{j-1} \d x_{j+1} \ldots \d x_p \]
\[= \int \cdots \int h_1 (x_1) \cdots h_p (x_p )  \d x_1 \ldots \d x_{j-1} \d x_{j+1} \ldots \d x_p   \]

\[  = h_j (x_j) = \prod_{i \neq j} \int h_i (x_i) \d x_i \implies   f_{X_j}(x_j) = c_j h_j (x_j) \mtext{ für eine Konstante } c_j   \]

Wegen $1 = \int f_{X_j} (x_j) \d x_j = c_j \int h_j (x_j) \d x_j$ ist $ c_j = \frac{1}{\int h_j (x_j) \d x_j}$.

Für alle $x_1, \ldots, x_p \in \R$ gilt:

\[ f_{X_1} (x_1), \cdots f_{X_p} (x_p) = \frac{ h_1 (x_1)}{\int h_1 (u_1) \d u_1} \cdots   \frac{ h_p (x_p)}{\int h_p (u_p) \d u_p}       \]
\[  = \frac{f(x_1, \ldots, x_p)}{\int \cdots \int h_1 (u_1) \cdots h_p (u_p) \d u_1 \ldots \d u_p }  =   \frac{f(x_1, \ldots, x_p)}{\int \cdots \int f(u_1, \ldots, u_p) \d u_1 \ldots \d u_p }  = f(x_1, \ldots, x_p )  \]

Daher sind $X_1, \ldots, X_p$ unabhängig. 

\newpage
% \section{30.10.13}

$\Cov (X,Y)$ ist ein Maß für den linearen Zusammenhang zwischen den beiden Zufallsvariablen $X$ und $Y$. Der Wert ist aber schwer interpretierbar, denn er ist maßstabsabhängig.

\[   \Cov(aX, bY) = ab \Cov(X,Y)\quad \forall a,b\in\R    \]

Man verwendet daher ein standardisiertes Zusammenhangsmaß, nämlich den Korrelationskoeffizienten. Der \emph{Korrelationskoeffizient} von Zufallsvariablen $X$ und $Y$ ist definiert durch $\rho (X,Y) = \frac{ \Cov(X,Y) }{\sqrt{\Var(X)\Var(Y)}}$, sofern $\Var(X), \Var(Y) > 0.$

$(\Var(X) = 0 \iff P(X=c)=1 \mtext{für ein} c\in\R$. $\rho$ ist maßstabsunabhängig:

\[ \rho(aX, bY) =   \frac{ab \Cov(X,Y) }{\sqrt{ a^2\Var(X) b^2 \Var(Y)}} = \frac{a}{\abs{a}} \frac{b}{\abs{b}} \rho(X,Y), \quad a,b \neq 0   \]
Vorzeichen können sich jedoch ändern.

\begin{satz} $X$ und $Y$ seien Zufallsvariablen mit $\Var(X)>0$ und $\Var(Y) > 0 \implies -1 \leq \rho(X,Y) \leq 1.$ \end{satz}

\[    \rho(X,Y) = 1 \iff \exists \alpha > 0, \beta\in\R \mtext{so dass} P(Y = \alpha X + \beta) = 1      \]
\[    \rho(X,Y) = -1 \iff \exists \alpha < 0, \beta\in\R \mtext{so dass} P(Y = \alpha X + \beta) = 1      \]

\begin{lemma} Cauchy-Schwarz Ungleichung. \end{lemma}

Für Zufallsvariablen $X$ und $Y$ gilt $(\E(XY))^2 \leq \E(X^2) \E(Y^2)$ und 

\[  (\E(XY))^2 = \E(X^2)\E(Y^2) \iff \exists a,b \neq 0 \in \R: P(aX=bY) = 1  \]

\begin{beweis} Setze $\alpha = \E(Y^2) \beta=\E(XY)$. \end{beweis} Behauptung klar, falls $\alpha =0$, da dann $P(Y=0)=1$ und $(\E(XY)) = 0 = \E(X^2)\E(Y^2)$ und es gilt $P(aX=bY)=1$ mit $a=0, b=1$.

Sei nun $\alpha$ nicht degeniert, also $\alpha \geq 0$.

\[ 0\leq  \E[(\alpha X - \beta Y)^2] = \alpha^2 \E(X^2) - 2\alpha\beta \underbrace{\E(XY)}_{=\beta} + \beta^2\underbrace{\E(Y^2)}_{=\alpha} \]
\[=   \alpha^2 \E(X^2) - \alpha\beta^2 = \alpha( \E(Y^2)\E(X^2) - (\E(XY))^2)  \underbrace{\implies}_{\alpha>0} (\E(XY))^2 \leq \E(X^2)\E(Y^2)  \]

Falls $(\E(XY))^2 = \E(X^2 \E(Y^2))$, dann

\[  \E[(\alpha X -\beta Y)^2] = 0 \implies P(\alpha X - \beta Y = 0) = 1 \mtext{also} P(aX=bY)=1 \mtext{mit} a = \alpha \neq 0, b = \beta        \]

Falls $P(aX = bY) = 1$ mit z.B. $a \neq 0$m dann $P(X=\frac{b}{a}Y)=1$ und 

\[  (\E(XY))^2 = (\E(\frac{b}{a}Y^2))^2   = \frac{b^2}{a^2} \E(Y^2) \E(Y^2) = \E(X^2)\E(Y^2)   \]


% \textbf{04.11.13}

Erinnerung Satz 9 und Lemma 10: 

\[-1 \leq \rho(X,Y) \leq 1\]
Cauchy Schwarz: \[(\E(XY))^2 \leq \E(X^2)\E(Y^2) \]


$X$ und $Y$ seien Zufallsvariablen mit $\Var(X) > 0, \Var (Y) > 0$

\[ (\rho(X,Y))^2 = \frac{\Cov(X,Y)^2}{\Var(X)\Var(Y)} = \frac{(\E[(X-\E(X)) (Y-\E(Y))])^2}{\Var(X)\Var(Y)}    \]

\[    \underbrace{\leq}_{\text{Lemma 10}} \frac{\E[(  X-\E(X) )^2]  \E[(Y-\E(Y))]^2  }{\Var(X)\Var(Y)} = 1         \]

\[ \abs{\rho(X,Y)} = 1 \underbrace{\implies}_{\text{Lemma 10}} = \exists \; a,b \in \R, \mtext{nicht beide} = 0, \mtext{mit}\]
\[ (*) P(a-(X-\E(X)) = b(Y-\E(Y)) ) = 1        \]

Hier muss zusätzlich $a \neq 0$ sein, denn sonst $P(Y=\E(Y)) = 1$ im Widerspruch zu $\Var(Y) > 0$. Ebenso muss $b \neq 0$ sein.

Also kann man $(*)$ schreiben als \[  P(Y = \underbrace{\frac{a}{b}}_{\alpha} X  \underbrace{- \frac{a}{b} \E(X) + \E(Y))}_{\beta}    \]

Damit gezeigt

\[   \abs{\rho(X,Y) = 1} \implies \exists \alpha \neq 0, \, \beta \in \R \mtext{mit} P(Y = \alpha X + \beta) = 1    \]
Bzgl. Vorzeichen von $\alpha$ : 

Falls $P(Y = \alpha X + \beta) = 1$ mit $\alpha \neq 0$, dann

\[   \rho(X,Y) = \rho(X, \alpha X + \beta) = \rho(X, \alpha X) =  \frac{\alpha}{\abs{\alpha}} \frac{\Cov(X,X)}{\Var(X)} = \begin{cases} 1, \; \alpha > 0 \\ -1, \; \alpha < 0. \end{cases}   \]\\

\begin{beispiel} Betrachte zweimaligen Wurf einer Münze mit $P(\text{'Kopf'}) = p \in (0,1)$.\end{beispiel}

Sei $X$ die Anzahl Würfe, in denen Kopf fällt und $Y$ der Rest. Berechnen nun $\rho(X,Y)$. Betrachte Erwartungswerte:

\[  \E(X) = 2p, \; \Var(X) = 2p(1-p), \; \E(Y) = 2(1-p), \; \Var(Y) = 2p(1-p), \]
\[ \E(XY) = 1 \cdot P(X=1, Y=1) = 2p(1-p)     \]
\[  \Cov(X,Y) = \E(XY) - \E(X)\E(Y) = 2p(1-p) - 4p(1-p) = -2p(1-p) \]
\[ \implies \rho(X,Y) = \frac{\Cov(X,Y)}{\sqrt{\Var(X)\Var(Y)}} = \frac{-2p(1-p)}{2p(1-p)} = -1  \]


Hier ist $P(X=\alpha X + \beta) = 1$ mit $\alpha = -1$ und $\beta = 2$. 

Die \emph{Korrelationsmatrix} eines Zufallsvektors $X (X_1, \ldots, X_p)^T$ ist definiert durch die Matrix der einzelnen Korrelationskoeffizienten 
\[  P = (\rho_jk)_{j,k = 1,\ldots,p} , \rho_{jk} = \rho(X_j, X_k) . \] sofern $\rho_j = \sqrt{\Var(X_j)}>0$ für alle $j=1,\ldots,p$.


$P$ ist die Kovarianzmatrix des Vektors der standardisierten Zufallsvariablen \(  Z_j = \frac{ X_j - \E(X_j) }{ \sigma_j  }  .  \)

Mit $\Delta = \diag(\sigma_1,\ldots, \sigma_p)$ gilt

\[ P = \Var ((Z_1, \ldots, Z_p)^T) = \Var( \Delta^{-1}(X-\E(X))) \underbrace{=}_{\text{Lemma 7}} \Delta^{-1} \Var(X) \Delta^{-1} .    \]

$X_j$ und $X_k$ heißen \emph{unkorreliert}, falls $\rho(X_j, X_k) = 0$.\\

\begin{satz}Transformationssatz für Dichten. \end{satz}

Sei $X$ ein $p$-dimensionaler Zufallsvektor mit Dichte $f_X$ (nicht Randdichte!).

Sei  $T = \{ x \in \R^p: f_X (x) > 0  \}$ Träger von $X$ und $M$ sei eine offene Teilmenge von $T$ mit $P(X \in M) = 1$.

Sei $h: M \to h(M) \subset \R^p$ eine bijektive Abbildung, stetig differenzierbar mit $J(x) = \det{\mathrm{D}h(x)} = \det{\frac{\partial h_i (x)}{\partial x_j}}_{i,j = 1,\ldots,p} \neq 0 \; \forall x \in M$. $\mathrm{D}$ ist Differentialoperator.

Dann hat $Y = h(X)$ die Dichte

\[ f_Y (y) = \begin{cases} f_X (h^{-1} (y)) \abs{\det (\mathrm{D}h^-1 (y))} = \frac{f_X (h^-1 (y))}{\abs{J(h^-1(y))}}, \; y \in h(M)   \\ 0 \mtext{sonst.}    \end{cases}  \]


\begin{beispiel} Log-Normalverteilung. \end{beispiel}

Sei $X \tilde N(\mu, \sigma^2)$, also \[f_X (x) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp{(-\frac{(x-\mu)^2}{2\sigma^2}), \; x \in \R}.\]

Hier ist $p=1,\, T = \R,\, M = \R,\, h(x)=e^x,\, h(M) = (0, \infty), \, h^-1(y) = \log y$, Funktionaldeterminante $J(x) = h'(x) = e^x \neq 0 \; \forall x \in M$.

\(   \implies Y = h(X)  \) hat die Dichte:

\[  f_Y (y) = \frac{ f_X (h^{-1}(y))}{\abs{J(h^-1 (y))}} = \frac{f_X (\log y)}{\abs{J(\log y)}}  = \frac{1}{\sqrt{2\pi\sigma^2}} \exp(-\frac{ (\log y - \mu)^2 }{2\sigma^2}) \frac{1}{y}    \]

\begin{beispiel} Sei $X = (X_1, X_2)^T$ ein 2-dimensionaler Zufallsvektor mit Dichte $f_X$ und es sei $T = \{ x\in\R^2 : f_X (x) > 0 \}$ offen. \end{beispiel}

Bestimme Dichte von $Y = \begin{pmatrix} X_1 + X_2 \\X_1 - X_2 \end{pmatrix}$.

Setze $M=T$ und 

\[ h(x) = \begin{pmatrix} h_1(x) \\ h_2 (x)   \end{pmatrix} = \begin{pmatrix} x_1 + x_2 \\ x_1 - x_2 \end{pmatrix} \mtext{für} x = (x_1, x_2)^T \in M \]

\[ J(x) = \det(  \frac{ \partial h_i (x) }{\partial x_j}    )_i,j = 1,2  = \det \begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix}  = 2 \neq 0    \]

\[  h(x) = (Y_1, Y_2)^T  \iff \begin{matrix} x_1 + x_2 = y_1 \\ x_1 - x_2 = y_2 \end{matrix}  \iff x_1 = \frac{y_1 + y_2}{2}, \; x_2 = \frac{y_1 - y_2}{2}   \]
Also 
\[  h^{-1}(y) = \begin{pmatrix} \frac{y_1 + y_2}{2} \\ \frac{y_1 - y_2}{2} \end{pmatrix}     \]

Insbesondere ist $h:M \to h(M)$ bijektiv.

$\implies Y$ hat die Dichte

\[ f_Y (y) = \begin{cases}  \frac{f_X (h^-1 (y))}{\abs{J(h^-1 (y))}} = \frac{1}{2}f_X (\frac{Y_1 + Y_2}{2}, \frac{Y_1 - Y_2}{2}), \; y \in h(M) \\ 0 \mtext{sonst.}  \end{cases}      \]

Falls $y \in \R^2 \setminus h(M)$, dann $h^-1 (y) \in \R^2 \setminus M$, also $f_X (h^-1 (y)) = 0$.

\[ f_Y (y) = \frac{1}{2} f_X \left(\frac{Y_1 + Y_2}{2}, \frac{Y_1 - Y_2}{2}\right), \; \forall \, y \in \R^2      \]
Wichtiger Spezialfall: Sind $X_1$ und $X_2$ unabhängig, also 

\[ f_X (x) = f_{X_1}(x_1) f_{X_2}(x_2) \mtext{ für alle } x \in \R      \]

Dann hat $X_1 + X_2$ die Dichte 

\[   f_{X_1 + X_2} (z) = f_{Y_1} (z) = \int\limits_{-\infty}^{\infty} f_Y (z, y_2) \d y_2  = \frac{1}{2} \int\limits_{-\infty}^{\infty} f_X \left(\frac{t + y_2}{2} , \frac{z - y^2}{2} \right) \d y_2  \]

\[   = \frac{1}{2} \int\limits_{-\infty}^{\infty} f_{X_1} \left(\frac{z - y^2}{2}\right) f_{X_2}\left(\frac{z-y^2}{2}\right) \d y_2 \underbrace{=}_{t = \frac{z + y_2}{2}}  \frac{1}{2} \int\limits_{-\infty}^{\infty} f_{X_1} (t) f_{X_2} \left(\frac{z - (2t - z)}{2}\right) 2 \d t  \]

\[  =  \int\limits_{-\infty}^{\infty} f_{X_1} (t) f_{X_2}(z-t) \d t    \]

Dies nennt man Faltung von $f_{X_1}$ und $f_{X_2}$.


% \section{06.11.13}

\begin{beispiel} Kunden erscheinen zu zufälligen Zeitpunkten in einem Laden. \end{beispiel} Der erste Kunde erscheint zur Zeit \(X_1\). Die Zeitspanne zwischen dem Eintreffen von Kunde \(i-1\) und Kunde \(i\) sei \(X_i, i=2,3,\dots\)\\
Dabei seien \(X_1,X_2,\dots\) unabhängige, exponentialverteilte Zufallsvariablen mit Erwartungswert \(\E[X_i] = \frac{1}{\lambda}, i=1,2,\dots\)\\
Bestimme die gemeinsame Dichte der Ankunftszeiten \(T_1,\dots,T_n\) der ersten \(n\) Kunden\\

\[T_i = X_1+\dots+X_i = \sum_i X_i.\]

$X_i$ hat Dichte $f_{X_i}(x_i) = \lambda e^{-\lambda x_i}, x_i > 0$

\( \Rightarrow X = \begin{pmatrix}X_1\\\vdots\\X_n\end{pmatrix} \) hat Dichte 

\[f_X(x_1,\dots,x_n) = \prod_{i=1}^n f_{X_i}(x_i) = \lambda^n e^{-\lambda \sum_{i=1}^n x_i}, x_1,\dots,x_n > 0\]

\(M = \{x \in \R^n : f_X(x) > 0\}\) ist offen.

\[ T = \begin{pmatrix}T_1\\\vdots\\T_n\end{pmatrix} = h(X) \mtext{mit} h_i(x_1,\dots,x_n) = x_1+ \dots ,x_n \]

\[ h(M) = \{t = \begin{pmatrix}t_1\\ \vdots \\t_n\end{pmatrix} \in \R^n : 0 < t_1 < t_2 < \dots < t_n\} \]

\[ t = h(x) \Leftrightarrow t_j = \sum_{i=1}^j x_i , j=1,\dots,n \Leftrightarrow x_1 = t_1 \wedge x_j = t_j - t_{j-1} , j=2,\dots,n \]

Also \[h^{-1}(t) = \begin{pmatrix}t_1\\t_2-t_1\\\vdots\\t_n-t_{n-1}\end{pmatrix}, J(x) = \det\left(\frac{\partial h_i(x)}{\partial x_j}\right)_{i,j=1,\dots,n} = \det\begin{pmatrix}1&0&0&\dots&0\\1&1&0&\dots&0\\\vdots& & &\ddots&\vdots\\1&1&1&\dots&1\end{pmatrix} = 1 \neq 0 \]

\[\implies \begin{pmatrix}T_1\\\vdots\\T_n\end{pmatrix}\] hat die Dichte:

\[ f_T(t_1,\dots,t_n) = \frac{f_X(h^{-1}(t_1,\dots,t_n))}{|J(h^{-1}(t_1,\dots,t_n))|} = \lambda^n e^{-\lambda (t_1+t_2-t_1+\dots+t_n-t_{n-1})} = \lambda^n e^{-\lambda t_n} , 0<t_1<\dots<t_n \]


Sei \(X = \begin{pmatrix}X_1\\X_2\end{pmatrix}\) ein \(p\)-dimensionaler Zufallsvektor, wobei \(X_1\) \(k\)-dimensional und \(X_2\) \((p-k)\)-dimensional.\\

Der \textbf{bedingte Erwartungswert} von \(g(X_1)\) gegeben \(X_2=x_2\) ist definiert durch:

\[ \E[g(X_1) | X_2=x_2] = \int g(x_1) f_{X_1}(x_1 | X_2=x_2) \d x_1 \]

Analog berechnet man die \textbf{bedingte Kovarianzmatrix} \(\Var(X_1 | X_2=x_2)\) unter Verwendung der bedingten Dichte \(f_{X_1}(x_1 | X_2=x_2)\).

\newpage
\subsection*{Übungsblatt 2}

\begin{aufgabe} 
\end{aufgabe}

Zeigen Sie: Ist $X$ eine $m \times n $ Zufallsmatrix und sind \(A \in \R^{k\times n}, B \in \R^{m\times l}, C \in \R^{k\times l}\), dann gilt:
\[ \E[AXB + C] = A\E[X]B + C\]

Sei \[A = (a_{ij})_{i=1,\dots,k; j=1,\dots,n},\] 
\[X = (x_{ij})_{i=1,\dots,n; j=1,\dots,m}, B = (b_{ij})_{i=1,\dots,m; j=1,\dots,l}, C = (c_{ij})_{i=1,\ldots, k;\; j=1,\dots,l}.\]

\[ \forall \, i=1,\dots,k; j=1,\dots,l: \{\E[AXB + C]\}_{ij} = \E[\{AXB+C\}_{ij}] \]
\[= \E \left[ \sum_{\mu=1}^n \sum_{\nu=1}^m a_{i \mu}X_{\mu \nu}B_{\nu j} + C_{ij} \right] = \sum_{\mu} \sum_{\nu} a_{i \mu} \E[X_{\mu \nu}] b_{\nu j} + c_{ij} = {A \E[X] B + C}_{ij} \]\\



\begin{aufgabe} 
\end{aufgabe}


Es sei \(X = (X_1,X_2,X_3)^T \) ein Zufallsvektor mit Kovarianzmatrix

\[ \Var(X) = \begin{pmatrix} 4 & 0 & -2 \\ 0 & 3 & 0 \\ -2 & 0 & 3 \end{pmatrix} \]

Berechnen Sie für jedes \(\alpha \in \R\) den Korrelationskoeffizienten

\[ \rho(\alpha X_1 - 7X_2 + X_3 + 2, \alpha X_1 + 7X_2 - X_3 -2) \]

Setze \[Y_1 = \alpha X_1 - 7X_2 + X_3 + 2, Y_2 = \alpha X_1 + 7X_2 - X_3 - 2, Y = \begin{pmatrix} Y_1 \\ Y_2 \end{pmatrix}.\]

\[ \Var(Y) = \Var \left( \begin{pmatrix}\alpha & -7 & 1 \\ \alpha & 7 & -1 \end{pmatrix} X + \begin{pmatrix}2\\-2\end{pmatrix} \right) = \begin{pmatrix}\alpha&-7&1\\\alpha&7&-1\end{pmatrix} \Var(X) \begin{pmatrix}\alpha&\alpha\\-7&7\\1&-1\end{pmatrix} = \ldots \]
\[= \begin{pmatrix}4\alpha^2-4\alpha+150 & 4\alpha^2-150 \\ & 4\alpha^2+4\alpha+150\end{pmatrix} = \begin{pmatrix} \Var(Y_1) & \Cov(Y_1,Y_2) \\ & \Var(Y_2)\end{pmatrix}\]

\[ \rho(Y_1,Y_2) = \frac{\Cov(Y_1,Y_2)}{\sqrt{\Var(Y_1)\Var(Y_2)}} \]
\[= \frac{4\alpha^2-150}{\sqrt{(4\alpha^2-4\alpha+150)(4\alpha^2+4\alpha+150)}} = \frac{2\alpha^2-75}{\sqrt{4\alpha^4+296\alpha^2+5625}} \]


\begin{aufgabe} 
\end{aufgabe}


Betrachte Sie \(n\) Ehepaare und nehmen Sie an, dass jede der \(2n\) Personen mit Wahrscheinlichkeit \(p \in (0,1)\) in 20 Jahren noch lebt (unabhängig von den anderen). Es sei \(X\) die Anzahl der in 20 Jahren noch lebenden Personen und \(Y\) sei die Anzahl der Ehepaare, bei denen in 20 Jahren sowohl die Frau als auch der Mann noch lebt. Berechnen Sie den Korrelationskoeffizienten von \(X\) und \(Y\).\\

Seien \(F_1,\dots,F_n, M_1,\dots,M_n\) unabhängige Bernoulli-Variablen.\\
\(F_i=1 \leftrightarrow\) Frau von Paar \(i\) lebt in 20 Jahren.\\
\(M_i=1 \leftrightarrow\) Mann von Paar \(i\) lebt in 20 Jahren.

\[ P(F_i=1) = P(M_i=1) = p, i=1,\dots,n \]
\[ X = \sum_{i=1}^n F_i + \sum_{i=1}^n M_i, Y = \sum_{i=1}^n F_iM_i \]
\[ X \sim \BIN(2n,p) \Rightarrow \E[X] = 2n \cdot p, \Var(X) = 2n \cdot p \cdot (1-p) \]
\[ P(F_iM_i = 1) = P(F_i=1) P(M_i=1) = p^2 \]
\[ \Rightarrow Y \sim \BIN(n,p^2) \Rightarrow \E[Y] = n \cdot p^2, \Var(Y) = n \cdot p^2 \cdot (1-p^2) \]
\[ \E[XY] = \E \left[ \left( \sum_{i=1}^n F_i + \sum_{i=1}^n M_i \right) \left( \sum_{j=1}^n F_jM_j \right) \right] =\]
\[ \sum_i \sum_j \E[F_iF_jM_j] + \E[M_iF_jM_j] = \sum_i \E[F_i^2M_i] + \E[M_i^2F_i] + \sum_i \sum_{j\neq i} \E[F_iF_jM_j] + \E[M_iF_jM_j] \]
\[= 2np^2 + 2n(n-1)p^3\]
Benutze dabei:
\[ \E[F_i^2M_i] = \E[F_iM_i] = \E[F_i] \E[M_i] = p^2 = \E[M_i^2F_i] \]
\[ \E[F_iF_jM_j] = \E[F_i] \E[F_j] \E[M_j] = p^3 = \E[M_iF_jM_j] \]

\[ \implies \rho(X,Y) = \frac{\Cov(X,Y)}{\sqrt{\Var(X)\Var(Y)}} = \frac{\E[XY] - \E[X]\E[Y]}{\sqrt{2np(1-p)np^2(1-p)^2}} \]
\[= \frac{2np^2+2n^2p^3-2np^3-2n^2p^3}{np\sqrt{2p(1-p)(1-p^2)}} = \frac{2p-2p^2}{(1-p)\sqrt{2p(1+p)}} = \sqrt{\frac{2p}{1+p}} \]


\begin{aufgabe} 
\end{aufgabe}


Es seien \(X_1, \dots, X_n\) unabhängig, identisch verteilte \(p\)-dimensionale Zufallsvektoren mit \(\E[X_1] = \mu\) und \(\Var(X_1) = \Sigma\). Es sei
\[ \bar{X} = \frac{1}{n} \sum_{j=1}^n X_j \]
\[ S = \frac{1}{n} \sum_{j=1}^n (X_j - \bar{X})(X_j - \bar{X})^T \]
Drücken Sie \(\E[\bar{X}]\), \(\Var(\bar{X})\) und \(\E[S]\) durch \(\mu\), \(\Sigma\) und \(n\) aus.

\[ \E[\bar{X}] = \E \left[ \frac{1}{n} \sum_j X_j \right] = \frac{1}{n} \sum_j \E[X_j] = \frac{1}{n} \sum_j \mu = \mu \]
\[ \Var(\bar{X}) = \Var \left( \frac{1}{n} \sum_j X_j \right) = \frac{1}{n^2} \sum_j \Var(X_j) = \frac{1}{n^2} \cdot n\Sigma = \frac{1}{n} \cdot \Sigma \]
\[ \E[S] = \E \left[ \frac{1}{n} \sum_j (X_j-\bar{X})(X_j-\bar{X})^T \right] = \frac{1}{n} \cdot \E \left[ \left( \sum_j X_jX_j^T \right) - \bar{X} \left( \sum_j X_j^T \right) - \left( \sum_j X_j \right) \bar{X}^T + n\bar{X}\bar{X}^T \right] \]
\[= \frac{1}{n} \cdot \sum_j \E [X_jX_j^T] - \E[\bar{X}\bar{X}^T] = \frac{1}{n} \sum_j (\Var(X_j) + \E[X_j] \cdot \E[X_j^T]) - \Var(\bar{X}) - \E[\bar{X}] \cdot \E^T[\bar{X}]\]
\[] = \Sigma + \mu\mu^T - \frac{1}{n} \Sigma - \mu\mu^T = \frac{n-1}{n} \Sigma \]


\subsection*{Übungsblatt 3}

\begin{aufgabe} 
\end{aufgabe}

Es seien \(X_1\) und \(X_2\) unabhängige exponentialverteilte Zufallsvariablen mit Parameter \(\lambda > 0\). Bestimmen Sie die gemeinsame Dichte von
\[ Y_1 = X_1 + X_2 , Y_2 = \frac{X_1}{X_2} \]
Sind \(Y_1\) und \(Y_2\) unabhängig?



Es seien \(X_i\) und \(T_i\) wie in Beispiel 15 der Vorlesung. Berechnen Sie für jedes \(t>0\)
\[ \E[X_1 | X_2 = t] \]


\begin{aufgabe} 
\end{aufgabe}


Der Zufallsvektor \(X = (X_1, X_2, X_3)^T \) habe die Dichte
\[ f(x_1,x_2,x_3) = \begin{cases} 2x_1 (x_2+x_3), & 0 \le x_i \le 1, i=1,2,3 \\ 0, & \mbox{sonst} \end{cases} \]

   (i)

   Berechnen Sie für jedes \(x_3 \in [0,1]\)
   \[ \E \left[ \begin{pmatrix}X_1\\X_2\end{pmatrix} |\, X_3 = x_3 \right] \]

   (ii)

   Berechnen Sie für jedes \(x_3 \in [0,1]\)
   \[ \Var \left[ \begin{pmatrix}X_1\\X_2\end{pmatrix} |\, X_3 = x_3 \right] \]


\newpage

% \section{11.11.13}

\[  X = (X_1, X_2)^T, \; \E[g(X_1) | X_2 = x_2] = \int g(x_1) f_{X_1} (x_1 |X_2 = x_2) \d x_1     \]

Die Funktion $ x_2 \to \E(X_1 | X_2 = x_2)$ wird als Regressionsfunktion bezeichnet für die Regression von $X_1$ auf $X_2$. Ist die Funktion von der Form $\alpha + \beta x_2$, so spricht man von linearer Regression.

Falls $X_1$ und $X_2$ unabhängig sind, dann gilt

\[   \E[g(X_1) | X_2 = x_2] = \int g(x_1) f_{X_1}(x_1 | X_2 = x_2) \d x_1 = \int g(x_1) f_{X_1} (x_1) \d x_1  = \E[g(X_1)] \quad \forall x_2      \]

und die Regressionsfunktion ist konstant.

\begin{beispiel}$(X_1, X_2)^T$ sei wie in Beispiel 5. Berechne Regressionsfunktion $\E(X_1 | X_2 = x_2)$ und $\Var(X_1| X_2 = x_2)$, $0 \leq x_2 \leq 1$.\end{beispiel}

\[ \E(X_1|X_2 = x_2) = \int x_1 f_{X_1} (x_1 | X_2 = x_2) \d x_1 = \int\limits_0^1 x_1 \frac{x_1 + x_2}{0.5 + x_2} \d x_1 \]
\[= \frac{1}{0.5 + x_2} \int\limits_0^1 x_1^2 + x_1 x_2 \d x_1 = \frac{\frac{1}{3} + \frac{x_2}{2}}{0.5 + x_2}     \]
\[  \E(X_1^2 | X_2 = x_2) = \frac{1}{0.5 + x_2} \int\limits_0^1 x_1^3 + x_1^2 x_2 \d x_1  = \frac{0.25 + \frac{x_2}{3}}{0.5 + x_2}    \]
\[   \implies \Var(X_1 | X_2 = x_2) = \frac{\frac{1}{4} + \frac{x_2}{3}}{\frac{1}{2} + x_2} - (\frac{\frac{1}{3} + \frac{x_2}{2}}{0.5 + x_2})^2 = \frac{1+ 6x_2 + 6x_2^2}{18(1 + 2x_2)^2} \]




Sei $X = (X_1, X_2)^T$ $p$-dimensionaler Zufallsvektor mit Dichte $f$, $X_1$ $k$-dimensional, $X_2$ $(p-k)$-dimensional.

Setze \[  \psi (x_2) = \E(X_1 | X_2 = x_2) \; \forall x_2 \in \R^{p-k} : f_{X_2} (x_2) < 0     \]

Der $k$-dimensionale Zufallsvektor $\psi (X_2)$ hat folgende Eigenschaften:

\begin{enumerate}

\item $ \E[\psi(X_2)] = \E(X_1)$
\item $ \psi(X_2) $ ist die beste Approximation von $X_1$ durch einen Zufallsvektor der Form $g(X_2)$ im Sinne der mittleren quadratischen Abweichung, d.h. für jede Funktion $g: \R^{p-k} \to \R^k$ gilt
\[  \E (\norm{X_1 - g(X_2)}^2 ) \geq \E (\norm{X_1 - \psi(X_2)}^2 ) \mtext{wobei} \norm{u} = \sqrt{u^T u} \forall u \in \R^k \]
\end{enumerate}
Begründung unter der Annahme $f_{X_2} (x_2) > 0 \; \forall x_2 \in \R^{p-k}$.\\

\begin{enumerate}
\item \[  \E[\psi (X_2)] = \int \psi (x_2) f_{X_2}(x_2) \d x_2  = \iint x_1 f_{X_1} (x_1 | X_2 = x_2) \d x_1 f_{X_2} (x_2) \d x_2    \]
\[  = \iint \frac{x_1 f(x_1, x_2)}{f_{X_2} (x_2) } f_{X_2} (x_2)   \d x_1 \d x_2 = \iint x_1 f(x_1, x_2) \d x_1 \d x_2 = \E(X_1)     \]
\item \[  \E(\norm{X_1 - g(X_2)}^2) = \E( \norm{X_1 - \psi (X_2) + \psi(X_2) - g(X_2)}^2 )     \] mit der Definition der euklidischen Norm folgt
\[  \E(\norm{X_1 - \psi(X_2)}^2) + \E(\norm{\psi(X_2) - g(X_2)}^2)  + 2\E[(X_1 - \psi(X_2))^T(\psi(X_2) - g(X_2))]    \]
\[  \E[(X_1 - \psi(X_2))^T (\psi(X_2) - g(X_2))] = \iint (x_1 - \psi (x_2))^T - (\psi(x_2) - g(x_2)) f(x_1, x_2) \d x_1 \d x_2           \]
\[   \int  \left[ \int (x_1 - \psi(x_2)) f(x_1, x_2) \d x_1 \right]^T  (\psi (x_2 ) - g(x_2)) \d x_2  \]
und für jedes $x_2 \in \R^{p-k}$ gilt
\[  \int (x_1 - \psi(x_2)) f(x_1, x_2) \d x_1 = \int x_1 f(x_1, x_2) \d x_1 - \int  \int t_1 f_{X_1}(t_1 | X_2 = x_2)     f(x_1, x_2) \d x_1     \] 
\[  \int x_1 f(x_1, x_2) \d x_1 - \int t_1 \underbrace{f_{X_1} (t_1 | X_2 = x_2) \underbrace{\int f(x_1, x_2) \d x_1}_{=f_{X_2} (x_2)}}_{= f(t_1, x_2)} \d t_1 = 0     \]
\[  \implies \E( \norm{X_1 - g(X_2)}^2 ) = \E(\norm{X_1 - \psi(X_2)}^2) + \E(\norm{\psi(X_2) - g(X_2)}^2)  \geq \E(\norm{X_1 - \psi(X_2)}^2) \blacksquare \]
\end{enumerate}

Der Zufallsvektor $\psi(X_2)$ wird auch mit $\E (X_1 | X_2)$ bezeichnet. Sind $X_1$ und $X_2$ unabhängig, dann gilt 
\[\psi(x_2) = \E (X_1 | X_2 = x_2) = \E(X_1)\] also $\E(X_1 | X_2) = E(X_1)$.


\section{Multivariate Normalverteilung}

\begin{lemma} Seien $X_1, \ldots, X_p$ unabhängige $N(0,1)$ verteilte Zufallsvariablen. \end{lemma}
\[  A \in \R^{p \times p} \mtext{regulär}, b \in\R^p, \; X = (X_1, \ldots, X_p)^T \]
\[ f_Y (y)  = \frac{1}{(2\pi)^{\frac{p}{2}}\sqrt{\det B}} \exp (-\frac{1}{2} (y-b)^T B^{-1} (y-b)), \; y \in \R^p  \]
wobei $B = AA^T$ und es gilt $E(Y) = b$, $\Var(Y) = B$.

\begin{beweis} \end{beweis} $X_i$ hat Dichte $f_{X_i} (x_i) = \frac{1}{\sqrt{2\pi}}e^{\frac{x_i^2}{2}}$ Daher hat $X$ die Dichte 
\[  f_X (x) = \prod_{i = 1}^p f_{X_i} (x_i) = \frac{1}{(2\pi)^{\frac{p}{2}}} e^{-\frac{1}{2} \sum_{i=1}^p x_i^2 } =  \frac{1}{(2\pi)^{\frac{p}{2}}} \exp (-\frac{1}{2} x^T x   \]
Wende Satz 1.12 an mit $h(x) = Ax + b$

\[  h^{-1} (y) = A^{-1} (y-b), \: J(x) = \abs{\det Dh(x)} = \abs{\det A}  \neq 0    \]
Daher $Y = h(X)$ hat die Dichte

\[    f_Y (y) = \frac{f_X (h^{-1}(y))}{\abs{J(h^{-1} (y))}} = \frac{1}{(2\pi)^{\frac{p}{2}}\abs{\det A}} \exp (-\frac{1}{2} (h^{-1}(y))^T h^{-1}(y))   \]
\[    = \frac{1}{(2\pi)^{\frac{p}{2}}\sqrt{\det B}} \exp (-\frac{1}{2} (y-b)^T (A^{-1})^T A^{-1} (y-b)  )         \]
\[ \underbrace{=}_{B^{-1} = (AA^{T})^{-1} = (A^{T})^{-1} A^{-1} = (A^{-1})^T A^{-1}} \E(Y) = A\E(X) + b = b   \]
\[ \Var(Y) = A\,\Var(X)A^T \underbrace{=}_{\Var(X) = \text{Einheitsmatrix}} AA^T = B   \]

% 13-11-2013
\newpage
\subsection{Übungsblatt 3} \textbf{Aufgabe 8}

$X = (X_1, X_2)^T$ hat die Dichte 
\[  f_X (x_1, x_2) = \begin{cases}  \lambda^2 e^{\lambda(x_1 + x_2)} & x_1, x_2 > 0 \\ 0 & \text{sonst} \end{cases}      \]
Wende Satz 1.12 an mit 
\[ M = (0, \infty), \; h(x_1, x_2) = (x_1 + x_2, x_1 / x_2)^T, \; x_1 + x_2 = y \mtext{ und} x_1 / x_2 = y_2     \]
\[ \iff x_2 y_2 + x_2 = y \mtext{und} x_1 = x_2 x_2 \iff x_2 = \frac{y_1}{1 + x_2} \mtext{und} \frac{y_1 y_1}{1 + y_2}     \]
\[ \mtext{also} h^{-1}(y) = \frac{y_1}{1 + y_2}, \; y = (y_1, y_2)^T \in h(M) = (0, \infty)^2     \]

\[  J(x) = \det Dh(x) = \det \begin{pmatrix}  1 & 1 \\ 1 / x_2 & x_1 / x_2   \end{pmatrix} =\frac{x_2}{x_2^2} - \frac{1}{x_2} = - \frac{x_1 + x_2}{x_2^2} \neq 0 \; \forall X \in ;\]
$  \implies y = (y_1, y_2)^T = h(X) $ hat die Dichte

\[  f_Y (y)  \frac{f_X (h^{-1}(y))}{\abs{J(h^{-1}(y))}} = \frac{\lambda^2 e^{-\lambda y_1}}{\abs{ - \frac{y_1}{y_1^2 / (1+y_2)^2}}}  \]
\[  = \begin{cases} \frac{\lambda^2 y_1 e^{-\lambda y_2}}{(1+y_2)^2} & y \in (0,\infty)^2 \\ 0 & \text{sonst} \end{cases}   \]

$ \underbrace{\implies}_{\text{Aufgabe 3}}$ $Y_1$ und $Y_2$ sind unabhängig.

Wähle z.B. $h_1(y_1) = \lambda^2 y_1 e^{-\lambda y_1} \1_{(0,\infty)}(y_1), \; h_2 (y_2) = \frac{1}{(1+y_2)^2} \1_{(0, \infty)} (y_2)$.

\textbf{Aufgabe 9}

$X_1, X_2$ seien unabhängige exponentialverteilte Zufallsvariablen mit Erwartungswert $\E(X_i) = \frac{1}{\lambda}$, $T_1 = X_1$, $T_2 = X_1 + X_2$.
Nach Beispiel 1.15 ist die Dichte von $T = (T_1, T_2)^T$ gegeben durch

\[  f_T(t_1, t_2) = \begin{cases} \lambda^2 e^{-\lambda t_2} & 0 < t_1 < t_2 \\ 0 & \text{sonst} \end{cases}  \]

Betrachte Randverteilung zur Berechnung der bedingten Dichte:

Für $t_2 > 0$ gilt \[f_{T_2}(t_2) = \int\limits_0^\infty f_T (t_1, t_2) \d t_1 = \int\limits_0^{t_2} \lambda^2 e^{-\lambda t_2} \d t_1 = t_2 \lambda^2 e^{-\lambda t_2}    \] und 
\[ \begin{cases} f_{T_1} (t_1 | T_2 = t_2) = \frac{f_T (t_1, t_2)}{f_T (t_2)} = \frac{\lambda^2 e^{-\lambda t_2}}{t_2 \lambda e^{-\lambda t_2}} = \frac{1}{t_2} & 0<t_1 < t_2  \\ 0 & \text{sonst} \end{cases}  \]
\[  \implies \E(X_1 | t_2 = t) = \E(T_1 | T_2 = t) = \int\limits_0^t  \frac{t_1}{t} \d t_1 = \frac{t}{2}, \; t>0.   \]


Alternativ: Heuristisches Argument
\[ \E(X_1 | T_2 = t) = \E(X_2 | T_2 = t) \mtext{wegen Symmetrie}     \]
\[ \E(X_1 | T_2 = t) + \E(X_2 | T_2 = t) = \E(X_1 + X_2 | T_2 = t) = \E(T_2 | T_2 = t) = t    \]
\[ \implies \E(X_1 | T_2 = t) = \E(X_2 | T_2 = t) = \frac{t}{2} \; \blacksquare  \]


\textbf{Aufgabe 10}

Für $x_3 \in [0,1]$ gilt
\[ f_{X_3} (x_3) = \iint f(x_1, x_2, x_3) \d x_1 \d x_2 = \int\limits_0^1 \int\limits_0^1 2 x_1 x_2 + x_1 x_3 \d x_1 \d x_2 = 2 \int\limits_0^1 x_1 \d x_1 \int\limits_0^1 x_2 \d x_2 + 2 x_3 \int\limits_0^1 x_1 \d x_1  \]
\[ = 2 \frac{1}{2} \frac{1}{2} + 2 x_3 \frac{1}{2} = \frac{1}{2} + x_3     \]
Und

\[  f_{(X_1, X_2)^T} (x_1, x_2 | X_3 = x_3) = \frac{f(x_1, x_2, x_3)}{f_{X_3}(x_3)} = \frac{2 x_1 x_2 + 2 x_1 x_3}{0.5 + x_3}, \; x_1 , x_2 \in [0,1]    \]

a) \[  \E[(X_1, X_2)^T | X_3 = x_3] =  \int\limits_0^1 \int\limits_0^1 (X_1, X_2)^T \frac{4x_1 x_2 + 4 x_1 x_3}{1 + 2 x_3} \d x_1 \d x_2  \]
\[  = \frac{4}{1 + 2x_3}  \begin{pmatrix}   \int\limits_0^1 x_1^2 \d x_1 \int\limits_0^1 x_2 \d x_2 + x_3 \int\limits_0^1 x_1^2 \d x_1    \\
\int\limits_0^1 x_1 \d x_1 \int\limits_0^1 x_2^2 \d x_2 + x_3 \int\limits_0^1 x_1 \d x_1 \int\limits_0^1 x_2 \d x_2 \end{pmatrix} \]
\[ \left( \frac{2}{3}, \frac{2 + 3x_3}{3 + 6 x_3}  \right)^T  \]

b) Analog zu a)
\[ \E[(X_1^2, X_2^2)^T | X_3 = x_3] = \int\limits_0^1 \int\limits_0^1  (X_1, X_2)^T \frac{4x_1 x_2 + 4 x_1 x_3}{1 + 2 x_3} \d x_1 \d x_2 = \cdots = \begin{pmatrix}  \frac{1}{2} \\ \frac{3 + 4x_3}{6 + 12 x_3}  \end{pmatrix}     \]
\[ f_{(X_1, X_2)^T} (x_1, x_2 | X_3 = x_3) = 2 x_1 \1_{[0,1]}(x_1) \frac{x_2 + x_3}{0.5 + x_3} \1_{[0,1]} (x_2)  \]
\[ \underbrace{\implies}_{\text{Aufgabe 3}} \E(X_1 \cdot X_2 | X_3 = x_3) = \E(X_1 | X_3 = x_3) \E(X_2 | X_3 = x_3)      \]
\[ \implies \Var[(X_1, X_2)^T | X_3 = x_3  ]  = \begin{pmatrix} \frac{1}{2} - (\frac{2}{3})^2 & 0 \\   0 &  \frac{3 + 4x_3}{6 + 12 x_3} - (\frac{2 + 3 x_3}{3 + 6x_3})^2     \end{pmatrix}  \]
\[  = \begin{pmatrix} \frac{1}{18}  & 0\\   0 &  \frac{1 + 6 x_3 + 6 x_3^2}{18(1 + 2x^3)^2}   \end{pmatrix}    \]

Ergänzung außerhalb der Vorlesung:
Eine quadratische Matrix $A$ heisst \emph{regulär}, falls ihre Determinante von 0 verschieden ist, also $det A \neq 0$. Andernfalls heißt A singulär, nämlich falls $\det A = 0$.

Für $A \in \R^{m \times n}$ gilt: $A$ regulär 
$\iff$ $A$ hat eine Inverse $A^{-1}$ 
$\iff$ Für jedes $b \in \R^{n}$ existiert genau ein $x \in \R^n$ mit $Ax = b$
$\iff \{ Ax: \; x \in \R^{n} \} = \R^n  $
$\iff Ax = 0, \; x \in \R^n, $ gilt nur für $x = 0$
$\iff A^T $ ist regulär

Für jede reguläre Matrix $A$ gilt $(A^T)^{-1} = (A^{-1})^T$.

Sind $A,B \in \R^{n \times m} $ regulär, dann ist $AB$ auch regulär und $(AB)^{-1} = B^{-1} A^{-1}$. 

\textbf{Definition 2} Eine symmetrische Matrix $A \in \R^{p \times p}$ heisst \emph{positiv definit} (Schreibweise in Zukunft $A>0$), falls $x^T A x > 0$ für alle $x \in \R^p \setminus \{0\}$. $A$ heisst \emph{nicht negativ definit}, falls $x^T A x \geq 0$ für alle $x \in \R^p$ (in Zukunft $A \geq 0)$.

\textbf{Bemerkung 3} a) Jede Kovarianzmatrix ist symmetrisch und nicht-negativ definit (Lemma 1.7 f)).
b) Ist $X$ ein $p$-dimensionaler Zufallsvektor und ist $\Var(X)$ nicht positiv definit, dann existiert $a \in \R^p, \; a \neq 0$ mit $a^T \Var(X)a = 0$.

\[ \underbrace{\implies}_{\text{Lemma 1.7c}} \Var(a^T X) = 0 \implies P((a^T X) = a^T \E(X)) = 1   \]

Das heisst $X$ liegt mit Wahrscheinlichkeit 1 in einer Hyperebene.

c) Aus $A \geq 0$ und $A \neq 0$ folgt nicht $A > 0$ (Lemma 1.7f)). Zum Beispiel gilt für $A = \begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix}$, $A\neq 0$, $(x_1, x_2) A (x_1, x_2)^T = (x_1 + x_2)^2 \geq 0$ für alle $x_1, x_2 \in \R$. Aber $(1, -1) A (1, -1)^T = 0$, d.h. ``$A>0$'' gilt nicht.

d) Für jede symmetrische nicht-negativ definite Matrix $A \in \R^{p \times p}$ gilt $ A > 0 \iff$ $A$ ist regulär. 

``$\implies$'' $ x^T A x > 0$ für alle $x \neq 0$ $\implies Ax \neq 0$ für alle $x \neq 0$ $\implies A$ regulär. 

``$\impliedby$'' Sei $A$ regulär und für $x \in \R^p$ gelte $x^T A x = 0$. Zeige $x=0$. Für $y \in \R^p$ gelte $p_y (t) = (x+ty)` A (x+ty), \; t \in \R$. $\implies p_y(0)=0, \; p_y(t) \geq 0$ für alle $t \in \R$ $\implies 0 = p_y ` (0) = y^T A x + x^T A y = 2y^T A x$ also $y^T A x = 0$ für jedes $y \in \R^p$. $\implies Ax = 0 \underbrace{\implies}_{\text{$A$ regulär}} x = 0 \; \blacksquare$

$\mu \in \R^p, \; \Sigma \in \R^{p \times p}, \Sigma > 0$ 
Ein $p$-dimensionaler Zufallsvektor $X$ heisst $p$-dimensional normalverteilt, falls $X$ folgende Dichte hat:

\[ f_X (x) = \frac{1}{(2\pi)^\frac{p}{2} \sqrt{\det \Sigma}} \exp \left( -  \frac{1}{2}(x-\mu)^T \Sigma^{-1} (x-\mu)   \right), \; x \in \R^p  \]
\[ X \sim N_p (\mu, \Sigma)   \]
\end{document}
